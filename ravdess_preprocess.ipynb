{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f853a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured.\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "from pathlib import Path\n",
    "import re, json, math, random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Audio\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Paths & I/O\n",
    "DATA_DIR = Path(\"./data/RAVDESS\")\n",
    "OUT_DIR = Path(\"./preprocessed_dataset\")\n",
    "AUDIO_OUT_DIR = OUT_DIR / \"audio_16k_clean\"\n",
    "AUG_OUT_DIR = OUT_DIR / \"audio_16k_white_noise\"\n",
    "\n",
    "# Audio params\n",
    "TARGET_SR = 16000       # For Wav2Vec2\n",
    "MONO = True\n",
    "AUDIO_EXT = \".wav\"      # Change to \".flac\" if you prefer FLAC\n",
    "\n",
    "# Fade (usually off)\n",
    "FADE_IN_MS = 0          # set >0 only if you want a very small fade-in\n",
    "\n",
    "# Baseline noise (applied to CLEAN output to break initial silence)\n",
    "BASELINE_NOISE_ENABLE = True\n",
    "BASELINE_NOISE_SNR_DB = 32.0     # higher = lighter noise\n",
    "BASELINE_NOISE_START_ONLY = True # only add to the start\n",
    "BASELINE_NOISE_START_SEC = 0.5   # duration at the clip start to receive noise\n",
    "\n",
    "# White-noise augmentation (a separate DUPLICATE file)\n",
    "AUGMENT_WRITE_DUPLICATE = True\n",
    "AUG_SNR_DB = 28.0                 # slightly stronger than baseline\n",
    "\n",
    "# Randomness\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Make dirs\n",
    "AUDIO_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if AUGMENT_WRITE_DUPLICATE:\n",
    "    AUG_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d4a7966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocal': {'speech': 0, 'song': 1},\n",
       " 'emotion': {'neutral': 0,\n",
       "  'calm': 1,\n",
       "  'happy': 2,\n",
       "  'sad': 3,\n",
       "  'angry': 4,\n",
       "  'fearful': 5,\n",
       "  'disgust': 6,\n",
       "  'surprised': 7},\n",
       " 'intensity': {'normal': 0, 'strong': 1}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_pattern = re.compile(r\"^(\\d{2})-(\\d{2})-(\\d{2})-(\\d{2})-(\\d{2})-(\\d{2})-(\\d{2})\\.wav$\", re.I)\n",
    "\n",
    "emotion_map = {\n",
    "    \"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\",\n",
    "    \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\"\n",
    "}\n",
    "intensity_map = {\"01\": \"normal\", \"02\": \"strong\"}\n",
    "vocal_map = {\"01\": \"speech\", \"02\": \"song\"}\n",
    "\n",
    "label2id = {\n",
    "    \"vocal\": {name: i for i, name in enumerate([\"speech\", \"song\"])},\n",
    "    \"emotion\": {name: i for i, name in enumerate(\n",
    "        [\"neutral\",\"calm\",\"happy\",\"sad\",\"angry\",\"fearful\",\"disgust\",\"surprised\"]\n",
    "    )},\n",
    "    \"intensity\": {name: i for i, name in enumerate([\"normal\",\"strong\"])},\n",
    "}\n",
    "label2id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f8ce121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 1440\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>actor</th>\n",
       "      <th>gender</th>\n",
       "      <th>vocal</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/RAVDESS/Actor_01/03-01-04-02-01-01-01.wav</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "      <td>speech</td>\n",
       "      <td>sad</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/RAVDESS/Actor_01/03-01-07-02-02-01-01.wav</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "      <td>speech</td>\n",
       "      <td>disgust</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/RAVDESS/Actor_01/03-01-07-02-01-02-01.wav</td>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "      <td>speech</td>\n",
       "      <td>disgust</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             path actor gender   vocal  \\\n",
       "0  data/RAVDESS/Actor_01/03-01-04-02-01-01-01.wav    01   male  speech   \n",
       "1  data/RAVDESS/Actor_01/03-01-07-02-02-01-01.wav    01   male  speech   \n",
       "2  data/RAVDESS/Actor_01/03-01-07-02-01-02-01.wav    01   male  speech   \n",
       "\n",
       "   emotion intensity  \n",
       "0      sad    strong  \n",
       "1  disgust    strong  \n",
       "2  disgust    strong  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for actor in sorted(DATA_DIR.glob(\"Actor_*\")):\n",
    "    for f in actor.glob(\"*.wav\"):\n",
    "        m = file_pattern.match(f.name)\n",
    "        if m:\n",
    "            modality, vocal, emotion, intensity, stmt, rep, actor_id = m.groups()\n",
    "            actor_num = int(actor_id)\n",
    "            gender = \"male\" if actor_num % 2 == 1 else \"female\"\n",
    "            rows.append({\n",
    "                \"path\": f,\n",
    "                \"actor\": actor_id,\n",
    "                \"gender\": gender,\n",
    "                \"vocal\": vocal_map.get(vocal, vocal),\n",
    "                \"emotion\": emotion_map.get(emotion, emotion),\n",
    "                \"intensity\": intensity_map.get(intensity, intensity),\n",
    "            })\n",
    "        else:\n",
    "            print(\"Filename not matching pattern:\", f)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"Total files found: {len(df)}\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d0c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path: Path):\n",
    "    # Returns float32 tensor in [-1, 1] when possible\n",
    "    wav, sr = torchaudio.load(str(path))   # [C, T]\n",
    "    if wav.dtype != torch.float32:\n",
    "        wav = wav.float() / (2**15)\n",
    "    return wav, sr\n",
    "\n",
    "def to_mono(wav: torch.Tensor):\n",
    "    # [C, T] -> [1, T]\n",
    "    if wav.ndim == 2 and wav.size(0) > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "    return wav\n",
    "\n",
    "def resample(wav: torch.Tensor, sr_in: int, sr_out: int):\n",
    "    if sr_in == sr_out:\n",
    "        return wav\n",
    "    resampler = T.Resample(sr_in, sr_out)\n",
    "    return resampler(wav)\n",
    "\n",
    "def apply_fade_in(wav: torch.Tensor, sr: int, fade_ms: int):\n",
    "    if fade_ms <= 0:\n",
    "        return wav\n",
    "    fade_samples = int(sr * fade_ms / 1000.0)\n",
    "    fade_samples = max(1, min(fade_samples, wav.size(-1)))\n",
    "    envelope = torch.linspace(0.0, 1.0, fade_samples, dtype=wav.dtype, device=wav.device)\n",
    "    wav[..., :fade_samples] *= envelope\n",
    "    return wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a4baac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def mix_white_noise(\n",
    "    wav: torch.Tensor,\n",
    "    snr_db: float,\n",
    "    apply_to_start_only: bool = False,\n",
    "    start_sec: float = 0.5,\n",
    "    sr: int = 16000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Add white noise at a target SNR (dB).\n",
    "    - If apply_to_start_only=True, only first `start_sec` seconds receive noise.\n",
    "    - Otherwise, noise is added to the entire clip.\n",
    "    \"\"\"\n",
    "    x = wav[0] if wav.ndim == 2 else wav\n",
    "\n",
    "    # Signal power (avoid zero)\n",
    "    sig_pow = torch.mean(x**2).item() + 1e-12\n",
    "    noise_pow = sig_pow / (10.0 ** (snr_db / 10.0))\n",
    "    noise_std = math.sqrt(noise_pow)\n",
    "    noise = torch.randn_like(x) * noise_std\n",
    "\n",
    "    if apply_to_start_only:\n",
    "        n = min(int(start_sec * sr), x.numel())\n",
    "        x[:n] = (x[:n] + noise[:n]).clamp(-1.0, 1.0)\n",
    "    else:\n",
    "        x = (x + noise).clamp(-1.0, 1.0)\n",
    "\n",
    "    return x.unsqueeze(0) if wav.ndim == 2 else x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "174a15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one(row, write_aug=AUGMENT_WRITE_DUPLICATE):\n",
    "    in_path: Path = row[\"path\"]\n",
    "    rel = in_path.relative_to(DATA_DIR)\n",
    "\n",
    "    out_clean = (AUDIO_OUT_DIR / rel).with_suffix(AUDIO_EXT)\n",
    "    out_clean.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_aug = None\n",
    "    if write_aug:\n",
    "        out_aug = (AUG_OUT_DIR / rel).with_suffix(AUDIO_EXT)\n",
    "        out_aug.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load\n",
    "    wav_raw, sr_in = load_audio(in_path)\n",
    "\n",
    "    # Mono\n",
    "    if MONO:\n",
    "        wav_raw = to_mono(wav_raw)\n",
    "\n",
    "    # Resample to 16k\n",
    "    wav_16k = resample(wav_raw, sr_in, TARGET_SR)\n",
    "\n",
    "    # Optional tiny fade-in (off by default)\n",
    "    if FADE_IN_MS > 0:\n",
    "        wav_16k = apply_fade_in(wav_16k, TARGET_SR, FADE_IN_MS)\n",
    "\n",
    "    # Keep a clean base (no noise) for derived variants\n",
    "    wav_base = wav_16k.clone()\n",
    "\n",
    "    # CLEAN: baseline noise (typically start-only, very light) to break pure silence\n",
    "    if BASELINE_NOISE_ENABLE:\n",
    "        wav_clean = mix_white_noise(\n",
    "            wav_base.clone(),\n",
    "            snr_db=BASELINE_NOISE_SNR_DB,\n",
    "            apply_to_start_only=BASELINE_NOISE_START_ONLY,\n",
    "            start_sec=BASELINE_NOISE_START_SEC,\n",
    "            sr=TARGET_SR,\n",
    "        )\n",
    "    else:\n",
    "        wav_clean = wav_base\n",
    "\n",
    "    wav_clean = wav_clean.clamp(-1.0, 1.0)\n",
    "    torchaudio.save(str(out_clean), wav_clean, sample_rate=TARGET_SR, bits_per_sample=16)\n",
    "    dur_clean = wav_clean.size(-1) / TARGET_SR\n",
    "\n",
    "    # AUG: full-clip noise (stronger), from the pristine base\n",
    "    aug_path = None\n",
    "    dur_aug = None\n",
    "    if write_aug:\n",
    "        wav_aug = mix_white_noise(\n",
    "            wav_base.clone(),\n",
    "            snr_db=AUG_SNR_DB,\n",
    "            apply_to_start_only=False,\n",
    "            sr=TARGET_SR,\n",
    "        ).clamp(-1.0, 1.0)\n",
    "        torchaudio.save(str(out_aug), wav_aug, sample_rate=TARGET_SR, bits_per_sample=16)\n",
    "        aug_path = out_aug\n",
    "        dur_aug = wav_aug.size(-1) / TARGET_SR\n",
    "\n",
    "    return {\n",
    "        \"path_clean\": out_clean,\n",
    "        \"duration_clean_sec\": round(dur_clean, 4),\n",
    "        \"path_aug\": aug_path,\n",
    "        \"duration_aug_sec\": round(dur_aug, 4) if dur_aug is not None else None,\n",
    "        \"sr_in\": sr_in,\n",
    "        \"sr\": TARGET_SR,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2bc6c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1440 [00:00<?, ?it/s]/home/mahrjose/Hub/Research/HSA/.venv/lib/python3.12/site-packages/torchaudio/__init__.py:178: UserWarning: The 'bits_per_sample' parameter is not directly supported by TorchCodec AudioEncoder.\n",
      "  return save_with_torchcodec(\n",
      "100%|██████████| 1440/1440 [00:38<00:00, 37.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total manifest rows: 2880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor</th>\n",
       "      <th>gender</th>\n",
       "      <th>vocal</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "      <th>orig_sr</th>\n",
       "      <th>sr</th>\n",
       "      <th>path</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>augment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "      <td>speech</td>\n",
       "      <td>sad</td>\n",
       "      <td>strong</td>\n",
       "      <td>48000</td>\n",
       "      <td>16000</td>\n",
       "      <td>preprocessed_dataset/audio_16k_clean/Actor_01/...</td>\n",
       "      <td>3.8038</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "      <td>speech</td>\n",
       "      <td>sad</td>\n",
       "      <td>strong</td>\n",
       "      <td>48000</td>\n",
       "      <td>16000</td>\n",
       "      <td>preprocessed_dataset/audio_16k_white_noise/Act...</td>\n",
       "      <td>3.8038</td>\n",
       "      <td>white_noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "      <td>speech</td>\n",
       "      <td>disgust</td>\n",
       "      <td>strong</td>\n",
       "      <td>48000</td>\n",
       "      <td>16000</td>\n",
       "      <td>preprocessed_dataset/audio_16k_clean/Actor_01/...</td>\n",
       "      <td>4.0374</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01</td>\n",
       "      <td>male</td>\n",
       "      <td>speech</td>\n",
       "      <td>disgust</td>\n",
       "      <td>strong</td>\n",
       "      <td>48000</td>\n",
       "      <td>16000</td>\n",
       "      <td>preprocessed_dataset/audio_16k_white_noise/Act...</td>\n",
       "      <td>4.0374</td>\n",
       "      <td>white_noise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actor gender   vocal  emotion intensity  orig_sr     sr  \\\n",
       "0    01   male  speech      sad    strong    48000  16000   \n",
       "1    01   male  speech      sad    strong    48000  16000   \n",
       "2    01   male  speech  disgust    strong    48000  16000   \n",
       "3    01   male  speech  disgust    strong    48000  16000   \n",
       "\n",
       "                                                path  duration_sec  \\\n",
       "0  preprocessed_dataset/audio_16k_clean/Actor_01/...        3.8038   \n",
       "1  preprocessed_dataset/audio_16k_white_noise/Act...        3.8038   \n",
       "2  preprocessed_dataset/audio_16k_clean/Actor_01/...        4.0374   \n",
       "3  preprocessed_dataset/audio_16k_white_noise/Act...        4.0374   \n",
       "\n",
       "       augment  \n",
       "0         none  \n",
       "1  white_noise  \n",
       "2         none  \n",
       "3  white_noise  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    meta = process_one(row)\n",
    "    rec_base = {\n",
    "        \"actor\": row[\"actor\"],\n",
    "        \"gender\": row[\"gender\"],\n",
    "        \"vocal\": row[\"vocal\"],\n",
    "        \"emotion\": row[\"emotion\"],\n",
    "        \"intensity\": row[\"intensity\"],\n",
    "        \"orig_sr\": meta[\"sr_in\"],\n",
    "        \"sr\": meta[\"sr\"],\n",
    "    }\n",
    "    # Clean entry\n",
    "    records.append({\n",
    "        **rec_base,\n",
    "        \"path\": str(meta[\"path_clean\"]),\n",
    "        \"duration_sec\": meta[\"duration_clean_sec\"],\n",
    "        \"augment\": \"none\",  # note: may include tiny start-only baseline noise by design\n",
    "    })\n",
    "    # Augmented entry (if exists)\n",
    "    if meta[\"path_aug\"] is not None:\n",
    "        records.append({\n",
    "            **rec_base,\n",
    "            \"path\": str(meta[\"path_aug\"]),\n",
    "            \"duration_sec\": meta[\"duration_aug_sec\"],\n",
    "            \"augment\": \"white_noise\",\n",
    "        })\n",
    "\n",
    "manifest = pd.DataFrame(records)\n",
    "print(\"Total manifest rows:\", len(manifest))\n",
    "manifest.head(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f0d6a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet save failed (ok to ignore): Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
      "Saved: preprocessed_dataset/manifest.csv and config JSONs.\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "manifest_path_csv = OUT_DIR / \"manifest.csv\"\n",
    "manifest.to_csv(manifest_path_csv, index=False)\n",
    "\n",
    "# Optional Parquet\n",
    "try:\n",
    "    manifest_path_parquet = OUT_DIR / \"manifest.parquet\"\n",
    "    manifest.to_parquet(manifest_path_parquet, index=False)\n",
    "except Exception as e:\n",
    "    print(\"Parquet save failed (ok to ignore):\", e)\n",
    "\n",
    "with open(OUT_DIR / \"label2id.json\", \"w\") as f:\n",
    "    json.dump(label2id, f, indent=2)\n",
    "\n",
    "pre_cfg = {\n",
    "    \"target_sr\": TARGET_SR,\n",
    "    \"mono\": MONO,\n",
    "    \"fade_in_ms\": FADE_IN_MS,\n",
    "    \"baseline_noise\": {\n",
    "        \"enabled\": BASELINE_NOISE_ENABLE,\n",
    "        \"snr_db\": BASELINE_NOISE_SNR_DB,\n",
    "        \"start_only\": BASELINE_NOISE_START_ONLY,\n",
    "        \"start_sec\": BASELINE_NOISE_START_SEC,\n",
    "    },\n",
    "    \"augmentation_duplicate\": {\n",
    "        \"enabled\": AUGMENT_WRITE_DUPLICATE,\n",
    "        \"snr_db\": AUG_SNR_DB,\n",
    "    },\n",
    "    \"audio_ext\": AUDIO_EXT,\n",
    "}\n",
    "with open(OUT_DIR / \"preprocess_config.json\", \"w\") as f:\n",
    "    json.dump(pre_cfg, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", manifest_path_csv, \"and config JSONs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48caea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts by vocal:\n",
      "vocal\n",
      "speech    2880\n",
      "dtype: int64 \n",
      "\n",
      "Counts by emotion:\n",
      "emotion\n",
      "angry        384\n",
      "calm         384\n",
      "disgust      384\n",
      "fearful      384\n",
      "happy        384\n",
      "neutral      192\n",
      "sad          384\n",
      "surprised    384\n",
      "dtype: int64 \n",
      "\n",
      "Counts by augment:\n",
      "augment\n",
      "none           1440\n",
      "white_noise    1440\n",
      "dtype: int64 \n",
      "\n",
      "Duration stats (sec):\n",
      "count    2880.000000\n",
      "mean        3.700687\n",
      "std         0.336617\n",
      "min         2.936300\n",
      "25%         3.470200\n",
      "50%         3.670400\n",
      "75%         3.870600\n",
      "max         5.271900\n",
      "Name: duration_sec, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Counts by vocal:\")\n",
    "print(manifest.groupby(\"vocal\").size(), \"\\n\")\n",
    "\n",
    "print(\"Counts by emotion:\")\n",
    "print(manifest.groupby(\"emotion\").size(), \"\\n\")\n",
    "\n",
    "print(\"Counts by augment:\")\n",
    "print(manifest.groupby(\"augment\").size(), \"\\n\")\n",
    "\n",
    "print(\"Duration stats (sec):\")\n",
    "print(manifest[\"duration_sec\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bc6575a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_dataset/audio_16k_white_noise/Actor_04/03-01-02-01-01-02-04.wav exists: True\n",
      "preprocessed_dataset/audio_16k_white_noise/Actor_13/03-01-08-02-01-02-13.wav exists: True\n",
      "preprocessed_dataset/audio_16k_white_noise/Actor_20/03-01-08-01-02-02-20.wav exists: True\n",
      "preprocessed_dataset/audio_16k_white_noise/Actor_14/03-01-03-02-02-01-14.wav exists: True\n",
      "preprocessed_dataset/audio_16k_clean/Actor_10/03-01-06-01-01-01-10.wav exists: True\n"
     ]
    }
   ],
   "source": [
    "for p in manifest.sample(min(5, len(manifest)), random_state=SEED)[\"path\"]:\n",
    "    p = Path(p)\n",
    "    print(p, \"exists:\", p.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eb1ed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchcodec in ./.venv/lib/python3.12/site-packages (0.8.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!/home/mahrjose/Hub/Research/HSA/.venv/bin/python -m pip install torchcodec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
