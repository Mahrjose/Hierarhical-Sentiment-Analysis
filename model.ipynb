{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5739d6c0",
   "metadata": {},
   "source": [
    "# Hierarchical Semantic Voice Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae36214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahrjose/Hub/Research/HSA/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2Processor\n",
    "import json\n",
    "\n",
    "# Load manifest and label2id\n",
    "manifest = pd.read_csv(\"./preprocessed_dataset/manifest.csv\")\n",
    "with open(\"./preprocessed_dataset/label2id.json\") as f:\n",
    "    label2id = json.load(f)\n",
    "\n",
    "# Load Wav2Vec2 processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb58ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, manifest_df, label2id, processor):\n",
    "        self.df = manifest_df.reset_index(drop=True)\n",
    "        self.label2id = label2id\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row[\"path\"]\n",
    "\n",
    "        # Load audio (already preprocessed to 16k)\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        waveform = waveform.squeeze(0)  # [T]\n",
    "\n",
    "        # Process for Wav2Vec2 (padding happens later)\n",
    "        inputs = self.processor(\n",
    "            waveform.numpy(),\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\"\n",
    "        )\n",
    "\n",
    "        gender_id = self.label2id[\"gender\"][row[\"gender\"]]\n",
    "        emotion_id = self.label2id[\"emotion\"][row[\"emotion\"]]\n",
    "        intensity_id = self.label2id[\"intensity\"][row[\"intensity\"]]\n",
    "\n",
    "        return {\n",
    "            \"input_values\": inputs.input_values.squeeze(0),\n",
    "            # \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"gender\": torch.tensor(gender_id, dtype=torch.long),\n",
    "            \"emotion\": torch.tensor(emotion_id, dtype=torch.long),\n",
    "            \"intensity\": torch.tensor(intensity_id, dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15509470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_values = [b[\"input_values\"] for b in batch]\n",
    "\n",
    "    padded = processor.pad(\n",
    "        {\"input_values\": input_values},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True  # <-- ensures attention_mask exists\n",
    "    )\n",
    "\n",
    "    gender = torch.stack([b[\"gender\"] for b in batch])\n",
    "    emotion = torch.stack([b[\"emotion\"] for b in batch])\n",
    "    intensity = torch.stack([b[\"intensity\"] for b in batch])\n",
    "\n",
    "    return {\n",
    "        \"input_values\": padded.input_values,        # [B, T]\n",
    "        \"attention_mask\": padded.attention_mask,    # [B, T]\n",
    "        \"gender\": gender,\n",
    "        \"emotion\": emotion,\n",
    "        \"intensity\": intensity\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "323463bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gender'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m dataloader = DataLoader(\n\u001b[32m      4\u001b[39m     dataset,\n\u001b[32m      5\u001b[39m     batch_size=\u001b[32m4\u001b[39m,       \u001b[38;5;66;03m# you can adjust\u001b[39;00m\n\u001b[32m      6\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     collate_fn=collate_fn\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# quick test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[33m\"\u001b[39m\u001b[33minput_values\u001b[39m\u001b[33m\"\u001b[39m].shape)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgender:\u001b[39m\u001b[33m\"\u001b[39m, batch[\u001b[33m\"\u001b[39m\u001b[33mgender\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hub/Research/HSA/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hub/Research/HSA/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hub/Research/HSA/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mAudioDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Process for Wav2Vec2 (padding happens later)\u001b[39;00m\n\u001b[32m     19\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m.processor(\n\u001b[32m     20\u001b[39m     waveform.numpy(),\n\u001b[32m     21\u001b[39m     sampling_rate=sr,\n\u001b[32m     22\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     padding=\u001b[33m\"\u001b[39m\u001b[33mlongest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m gender_id = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgender\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[row[\u001b[33m\"\u001b[39m\u001b[33mgender\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     27\u001b[39m emotion_id = \u001b[38;5;28mself\u001b[39m.label2id[\u001b[33m\"\u001b[39m\u001b[33memotion\u001b[39m\u001b[33m\"\u001b[39m][row[\u001b[33m\"\u001b[39m\u001b[33memotion\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     28\u001b[39m intensity_id = \u001b[38;5;28mself\u001b[39m.label2id[\u001b[33m\"\u001b[39m\u001b[33mintensity\u001b[39m\u001b[33m\"\u001b[39m][row[\u001b[33m\"\u001b[39m\u001b[33mintensity\u001b[39m\u001b[33m\"\u001b[39m]]\n",
      "\u001b[31mKeyError\u001b[39m: 'gender'"
     ]
    }
   ],
   "source": [
    "dataset = AudioDataset(manifest, label2id, processor)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,       # you can adjust\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# quick test\n",
    "batch = next(iter(dataloader))\n",
    "print(batch[\"input_values\"].shape)\n",
    "print(\"gender:\", batch[\"gender\"])\n",
    "print(\"emotion:\", batch[\"emotion\"])\n",
    "print(\"intensity:\", batch[\"intensity\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3cd2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierarchicalAudioClassifier(\n",
       "  (encoder): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (speech_type_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=384, out_features=2, bias=True)\n",
       "  )\n",
       "  (emotion_heads): ModuleDict(\n",
       "    (speech): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=8, bias=True)\n",
       "    )\n",
       "    (song): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (intensity_heads): ModuleDict(\n",
       "    (neutral): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "    (calm): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "    (happy): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "    (sad): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "    (angry): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "    (fearful): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "    (disgust): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "    (surprised): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (pooling): AdaptiveAvgPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "\n",
    "class HierarchicalAudioClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 wav2vec_model_name=\"facebook/wav2vec2-base\", \n",
    "                 hidden_dim=768, \n",
    "                 num_speech_types=2, \n",
    "                 num_emotions=8, \n",
    "                 num_intensities=2):\n",
    "        \"\"\"\n",
    "        Hierarchical audio classification model using Wav2Vec2 as a feature extractor.\n",
    "        The model performs:\n",
    "            1. Speech Type classification (speech/song)\n",
    "            2. Emotion classification\n",
    "            3. Emotion intensity classification\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Feature Extractor ---\n",
    "        self.encoder = Wav2Vec2Model.from_pretrained(wav2vec_model_name)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # --- Level 1: Speech Type Classifier ---\n",
    "        self.speech_type_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, num_speech_types)\n",
    "        )\n",
    "        \n",
    "        # --- Level 2: Emotion Classifier ---\n",
    "        # We can have separate emotion heads for speech and song if desired\n",
    "        self.emotion_heads = nn.ModuleDict({\n",
    "            \"speech\": nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_dim // 2, num_emotions)\n",
    "            ),\n",
    "            \"song\": nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_dim // 2, num_emotions)\n",
    "            )\n",
    "        })\n",
    "        \n",
    "        # --- Level 3: Intensity Classifier ---\n",
    "        # Similarly, emotion-specific or global intensity heads can be defined\n",
    "        self.intensity_heads = nn.ModuleDict({\n",
    "            emotion: nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_dim // 2, num_intensities)\n",
    "            ) for emotion in [\n",
    "                \"neutral\", \"calm\", \"happy\", \"sad\", \n",
    "                \"angry\", \"fearful\", \"disgust\", \"surprised\"\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        # --- Global Pooling Layer ---\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward_features(self, audio_inputs, attention_mask=None):\n",
    "        \"\"\"Extract and pool features from Wav2Vec2.\"\"\"\n",
    "        outputs = self.encoder(audio_inputs, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # [B, T, H]\n",
    "        pooled = hidden_states.mean(dim=1)  # [B, H]\n",
    "        return pooled\n",
    "    \n",
    "    def forward(self, audio_inputs, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through all levels.\n",
    "        This function only returns raw logits for each level.\n",
    "        In practice, you’ll likely call them sequentially (level by level).\n",
    "        \"\"\"\n",
    "        pooled_features = self.forward_features(audio_inputs, attention_mask)\n",
    "        \n",
    "        # Level 1: Speech type logits\n",
    "        speech_logits = self.speech_type_head(pooled_features)\n",
    "        \n",
    "        # Placeholder outputs for emotion/intensity (computed conditionally in practice)\n",
    "        emotion_logits = None\n",
    "        intensity_logits = None\n",
    "        \n",
    "        return {\n",
    "            \"features\": pooled_features,\n",
    "            \"speech_logits\": speech_logits,\n",
    "            \"emotion_logits\": emotion_logits,\n",
    "            \"intensity_logits\": intensity_logits\n",
    "        }\n",
    "\n",
    "    def forward_emotion(self, features, speech_type_label: str):\n",
    "        \"\"\"Forward through the appropriate emotion head.\"\"\"\n",
    "        return self.emotion_heads[speech_type_label](features)\n",
    "    \n",
    "    def forward_intensity(self, features, emotion_label: str):\n",
    "        \"\"\"Forward through the appropriate intensity head.\"\"\"\n",
    "        return self.intensity_heads[emotion_label](features)\n",
    "\n",
    "model = HierarchicalAudioClassifier()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HierarchicalAudioClassifier().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28b548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "inputs = batch[\"input_values\"].to(device)\n",
    "mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "out = model(inputs, attention_mask=mask)\n",
    "print(out[\"speech_logits\"].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
